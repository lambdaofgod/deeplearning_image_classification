{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "from deeplearning_image_classification import training\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weights.01-3.82.hdf5',\n",
       " 'weights.02-3.58.hdf5',\n",
       " 'weights.03-3.30.hdf5',\n",
       " 'weights.04-3.25.hdf5',\n",
       " 'weights.05-3.20.hdf5',\n",
       " 'weights.06-3.07.hdf5',\n",
       " 'weights.07-3.06.hdf5',\n",
       " 'weights.08-2.97.hdf5',\n",
       " 'weights.09-2.93.hdf5',\n",
       " 'weights.10-2.86.hdf5',\n",
       " 'weights.11-2.83.hdf5',\n",
       " 'weights.12-2.80.hdf5',\n",
       " 'weights.13-2.80.hdf5',\n",
       " 'weights.14-2.80.hdf5',\n",
       " 'weights.15-2.69.hdf5',\n",
       " 'weights.16-2.79.hdf5',\n",
       " 'weights.17-2.80.hdf5',\n",
       " 'weights.18-2.73.hdf5',\n",
       " 'weights.19-2.69.hdf5',\n",
       " 'weights.20-2.72.hdf5']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "files = os.listdir()\n",
    "print(files)\n",
    "\n",
    "weight_files = [f for f in files if f.startswith('weights')]\n",
    "last_weights = sorted(weight_files)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 validated image filenames belonging to 82 classes.\n",
      "Found 1000 validated image filenames belonging to 82 classes.\n",
      "Found 10000 validated image filenames belonging to 82 classes.\n",
      "['download_data.sh', 'results.csv', 'deeplearning_image_classification.egg-info', 'data', '.ipynb_checkpoints', 'MANIFEST.in', 'docs', 'README.md', 'run.log', '00_core.ipynb', '.github', 'deeplearning_image_classification', 'CONTRIBUTING.md', 'prepare_data.sh', 'runs_rounded.csv', 'runs.csv', '.gitconfig', '.gitignore', 'guild.yml', 'requirements.txt', 'scripts', 'index.ipynb', 'setup.py', 'LICENSE', 'final_results.csv', 'settings.ini', 'notebooks', '.git', 'Makefile']\n",
      "WARNING:tensorflow:LossScale of LossScaleOptimizer passed to compile (DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)) is not the same as the dtype policy's loss scale (DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)). Because the dtype policy has a loss scale, you should pass an optimizer that is not wrapped with a LossScaleOptimizer,\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "209/209 [==============================] - 58s 275ms/step - loss: 2.9133 - acc: 0.3534 - top_k_categorical_accuracy: 0.5817 - precision: 0.6763 - recall: 0.2453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.9133148923444976, 0.3534, 0.5817, 0.6763165, 0.2453]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "sample_size = 'all' \n",
    "val_size = 10000\n",
    "image_size = (224, 224)\n",
    "train_val_metadata_df, test_metadata_df = training.get_metadata()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if sample_size != 'all':\n",
    "        __, train_val_metadata_df = model_selection.train_test_split(\n",
    "            train_val_metadata_df,\n",
    "            test_size=sample_size,\n",
    "            random_state=2,\n",
    "            stratify=train_val_metadata_df['class']\n",
    "        )\n",
    "    train_metadata_df, val_metadata_df = model_selection.train_test_split(\n",
    "        train_val_metadata_df,\n",
    "        test_size=val_size,\n",
    "        random_state=2,\n",
    "        stratify=train_val_metadata_df['class']\n",
    "    )\n",
    "\n",
    "    train_image_iterator, val_image_iterator, test_image_iterator = training.get_train_val_test_iterators(\n",
    "        train_metadata_df, val_metadata_df, test_metadata_df,\n",
    "        image_size=image_size, train_batch_size=32, test_batch_size=48, val_batch_size=32) \n",
    "    n_classes = len(train_image_iterator.class_indices)\n",
    "\n",
    "    model = load_model(last_weights)\n",
    "    model.evaluate(test_image_iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
