{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow configuration.\n",
    "\n",
    "Without setting memory growth TF 2 runs into problems, see [this issue](https://github.com/tensorflow/tensorflow/issues/24496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "float_dtype = 'float16'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from deeplearning_image_classification import data_loading\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "np.random.seed(0)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "DATA_DIR = data_loading.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "if float_dtype == 'float16': \n",
    "    tf.keras.backend.set_floatx('float16')\n",
    "    tf.keras.backend.set_epsilon(1e-4)\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/deeplearning_image_classification\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try removing this line on your PC, maybe you will not encounter [this issue](https://github.com/tensorflow/tensorflow/issues/24496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "train_csv_path = os.path.join(DATA_DIR, 'train_metadata.csv')\n",
    "test_csv_path = os.path.join(DATA_DIR, 'test_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_metadata(train_metadata_path=train_csv_path, test_metadata_path=test_csv_path):\n",
    "    train_metadata_df = pd.read_csv(train_csv_path)\n",
    "    test_metadata_df = pd.read_csv(test_csv_path)\n",
    "    return train_metadata_df, test_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train_metadata.csv'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df, test_metadata_df = get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 11000\n",
    "sample_val_size = 1000\n",
    "__, sample_train_val_metadata_df = model_selection.train_test_split(train_metadata_df, test_size=sample_size, random_state=2, stratify=train_metadata_df['class'])\n",
    "sample_train_metadata_df, sample_val_metadata_df = model_selection.train_test_split(sample_train_val_metadata_df, test_size=sample_val_size, random_state=2, stratify=sample_train_val_metadata_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "image_size = (224, 224)\n",
    "train_batch_size = 48 \n",
    "val_batch_size = 48\n",
    "test_batch_size = 48\n",
    "\n",
    "\n",
    "def get_train_val_test_iterators(train_metadata_df, val_metadata_df, test_metadata_df, image_size, train_batch_size, val_batch_size, test_batch_size):\n",
    "    image_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=keras.applications.mobilenet.preprocess_input,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    train_image_iterator = image_gen.flow_from_dataframe(train_metadata_df, batch_size=train_batch_size, target_size=image_size,\n",
    "        shuffle=True)\n",
    "    val_image_iterator = image_gen.flow_from_dataframe(val_metadata_df, batch_size=val_batch_size, target_size=image_size, shuffle=False)\n",
    "    test_image_iterator = image_gen.flow_from_dataframe(test_metadata_df, batch_size=test_batch_size, target_size=image_size, shuffle=False)\n",
    "    return train_image_iterator, val_image_iterator, test_image_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 validated image filenames belonging to 82 classes.\n",
      "Found 1000 validated image filenames belonging to 82 classes.\n",
      "Found 10000 validated image filenames belonging to 82 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_iterator, val_image_iterator, test_image_iterator = get_train_val_test_iterators(sample_train_metadata_df, sample_val_metadata_df, test_metadata_df, image_size=image_size, train_batch_size=train_batch_size, test_batch_size=test_batch_size, val_batch_size=val_batch_size) \n",
    "n_classes = len(train_image_iterator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up model\n",
    "\n",
    "We use pretrained MobileNet model for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "learning_rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "default_metrics = [\n",
    "    'acc',\n",
    "   keras.metrics.TopKCategoricalAccuracy(k=5),\n",
    "   keras.metrics.Precision(),\n",
    "   keras.metrics.Recall()\n",
    "]\n",
    "pretrained = False\n",
    "freeze_pretrained = False\n",
    "last_layer_convolutions = 64\n",
    "\n",
    "\n",
    "def setup_model(n_classes, optimizer, metrics):\n",
    "    weights =  'imagenet' if pretrained else None\n",
    "    base_model = keras_applications.mobilenet.MobileNet(weights=None, input_shape=(*image_size, 3), backend=tf.keras.backend, layers=tf.keras.layers, models=tf.keras.models, utils=tf.keras.utils) \n",
    "    if freeze_pretrained:\n",
    "        base_model.trainable = False\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(n_classes, dtype='float16'),\n",
    "            keras.layers.Softmax()\n",
    "        ])\n",
    "    model.compile(\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setup_model(n_classes, optimizer, default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 157 steps, validate for 16 steps\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 4.0691 - acc: 0.0840 - precision: 0.1087 - recall: 5.4538e-04WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 70s 448ms/step - loss: 4.0588 - acc: 0.0861 - precision: 0.0962 - recall: 5.0000e-04 - val_loss: 4.0149 - val_acc: 0.0300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 4.0430 - acc: 0.1007 - precision: 0.0836 - recall: 0.0028WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 67s 429ms/step - loss: 4.0445 - acc: 0.0997 - precision: 0.0938 - recall: 0.0033 - val_loss: 3.9540 - val_acc: 0.0300 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 4.0482 - acc: 0.1109 - precision: 0.0982 - recall: 0.0029WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 69s 437ms/step - loss: 4.0455 - acc: 0.1118 - precision: 0.1004 - recall: 0.0028 - val_loss: 3.9528 - val_acc: 0.0310 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 3.8148 - acc: 0.1277 - precision: 0.3977 - recall: 0.0038WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 3.8212 - acc: 0.1255 - precision: 0.3933 - recall: 0.0035 - val_loss: 3.9414 - val_acc: 0.0710 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.7717 - acc: 0.1396 - precision: 0.4895 - recall: 0.0076WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 64s 409ms/step - loss: 3.7734 - acc: 0.1391 - precision: 0.4966 - recall: 0.0073 - val_loss: 3.7980 - val_acc: 0.1270 - val_precision: 0.3415 - val_recall: 0.0140\n",
      "Epoch 6/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.7389 - acc: 0.1502 - precision: 0.5342 - recall: 0.0136WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 64s 410ms/step - loss: 3.7357 - acc: 0.1501 - precision: 0.5429 - recall: 0.0133 - val_loss: 3.7755 - val_acc: 0.1220 - val_precision: 0.3846 - val_recall: 0.0050\n",
      "Epoch 7/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.7126 - acc: 0.1545 - precision: 0.5606 - recall: 0.0176WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 64s 406ms/step - loss: 3.7104 - acc: 0.1536 - precision: 0.5643 - recall: 0.0180 - val_loss: 3.7970 - val_acc: 0.1330 - val_precision: 0.3625 - val_recall: 0.0290\n",
      "Epoch 8/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.6932 - acc: 0.1552 - precision: 0.5920 - recall: 0.0193WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 63s 402ms/step - loss: 3.6914 - acc: 0.1543 - precision: 0.6056 - recall: 0.0195 - val_loss: 3.7982 - val_acc: 0.1450 - val_precision: 0.5000 - val_recall: 0.0170\n",
      "Epoch 9/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.6693 - acc: 0.1609 - precision: 0.6337 - recall: 0.0259WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 65s 417ms/step - loss: 3.6706 - acc: 0.1611 - precision: 0.6259 - recall: 0.0266 - val_loss: 3.7837 - val_acc: 0.1280 - val_precision: 0.5000 - val_recall: 0.0080\n",
      "Epoch 10/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 3.6365 - acc: 0.1644 - precision: 0.6406 - recall: 0.0286WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 66s 421ms/step - loss: 3.6413 - acc: 0.1646 - precision: 0.6488 - recall: 0.0279 - val_loss: 3.8777 - val_acc: 0.1270 - val_precision: 0.3077 - val_recall: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0ab58f8c10>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_image_iterator, validation_data=val_image_iterator, epochs=10, use_multiprocessing=True, workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      " 85/157 [===============>..............] - ETA: 17s - loss: 3.8694 - acc: 0.1244 - precision: 0.2798 - recall: 0.0211"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_image_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(test_image_iterator).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([test_image_iterator.class_indices[c] for c in test_metadata_df['class']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "sample_size = 11000\n",
    "val_size = 1000\n",
    "epochs = 20 \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_val_metadata_df, test_metadata_df = get_metadata()\n",
    "    \n",
    "    if sample_size != 'all':\n",
    "        __, train_val_metadata_df = model_selection.train_test_split(\n",
    "            train_val_metadata_df,\n",
    "            test_size=sample_size,\n",
    "            random_state=2,\n",
    "            stratify=train_val_metadata_df['class']\n",
    "        )\n",
    "    train_metadata_df, val_metadata_df = model_selection.train_test_split(\n",
    "        train_val_metadata_df,\n",
    "        test_size=val_size,\n",
    "        random_state=2,\n",
    "        stratify=train_val_metadata_df['class']\n",
    "    )\n",
    "        \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "        monitor='val_top_k_categorical_accuracy'\n",
    "    )\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='logs', histogram_freq=0, write_graph=True, write_images=False,\n",
    "        update_freq=100, profile_batch=2, embeddings_freq=0,\n",
    "        embeddings_metadata=None\n",
    "    )\n",
    "    callbacks = [model_checkpoint_callback, tensorboard_callback]\n",
    "    train_image_iterator, val_image_iterator, test_image_iterator = get_train_val_test_iterators(\n",
    "        train_metadata_df, val_metadata_df, test_metadata_df,\n",
    "        image_size=image_size, train_batch_size=train_batch_size, test_batch_size=test_batch_size, val_batch_size=val_batch_size) \n",
    "    n_classes = len(train_image_iterator.class_indices)\n",
    "    model = setup_model(n_classes, optimizer, default_metrics)\n",
    "    model.fit(train_image_iterator, validation_data=val_image_iterator, epochs=epochs, callbacks=callbacks)\n",
    "    model.save('data/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
