{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from deeplearning_image_classification import data_loading\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.keras.backend.set_floatx('float16')\n",
    "tf.keras.backend.set_epsilon(1e-4)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import keras_applications\n",
    "#export\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "DATA_DIR = data_loading.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/deeplearning_image_classification\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try removing this line on your PC, maybe you will not encounter [this issue](https://github.com/tensorflow/tensorflow/issues/24496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "train_csv_path = os.path.join(DATA_DIR, 'train_metadata.csv')\n",
    "test_csv_path = os.path.join(DATA_DIR, 'test_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_metadata(train_metadata_path=train_csv_path, test_metadata_path=test_csv_path):\n",
    "    train_metadata_df = pd.read_csv(train_csv_path)\n",
    "    test_metadata_df = pd.read_csv(test_csv_path)\n",
    "    return train_metadata_df, test_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train_metadata.csv'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df, test_metadata_df = get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 11000\n",
    "sample_val_size = 1000\n",
    "__, sample_train_val_metadata_df = model_selection.train_test_split(train_metadata_df, test_size=sample_size, random_state=2, stratify=train_metadata_df['class'])\n",
    "sample_train_metadata_df, sample_val_metadata_df = model_selection.train_test_split(sample_train_val_metadata_df, test_size=sample_val_size, random_state=2, stratify=sample_train_val_metadata_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "image_size = (224, 224)\n",
    "train_batch_size = 64 \n",
    "val_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "\n",
    "def get_train_val_test_iterators(train_metadata_df, val_metadata_df, test_metadata_df, image_size, train_batch_size, val_batch_size, test_batch_size):\n",
    "    image_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=keras.applications.mobilenet.preprocess_input,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    train_image_iterator = image_gen.flow_from_dataframe(train_metadata_df, batch_size=train_batch_size, target_size=image_size,\n",
    "        shuffle=True)\n",
    "    val_image_iterator = image_gen.flow_from_dataframe(val_metadata_df, batch_size=val_batch_size, target_size=image_size, shuffle=False)\n",
    "    test_image_iterator = image_gen.flow_from_dataframe(test_metadata_df, batch_size=test_batch_size, target_size=image_size, shuffle=False)\n",
    "    return train_image_iterator, val_image_iterator, test_image_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 validated image filenames belonging to 82 classes.\n",
      "Found 1000 validated image filenames belonging to 82 classes.\n",
      "Found 10000 validated image filenames belonging to 82 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_iterator, val_image_iterator, test_image_iterator = get_train_val_test_iterators(sample_train_metadata_df, sample_val_metadata_df, test_metadata_df, image_size=image_size, train_batch_size=train_batch_size, test_batch_size=test_batch_size, val_batch_size=val_batch_size) \n",
    "n_classes = len(train_image_iterator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up model\n",
    "\n",
    "We use pretrained MobileNet model for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "learning_rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "default_metrics = ['acc', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "pretrained = False\n",
    "freeze_pretrained = False\n",
    "last_layer_convolutions = 64\n",
    "\n",
    "\n",
    "def setup_model(n_classes, optimizer, metrics):\n",
    "    weights =  'imagenet' if pretrained else None\n",
    "    base_model = keras_applications.mobilenet.MobileNet(weights=None, input_shape=(224, 224, 3), backend=tf.keras.backend, layers=tf.keras.layers, models=tf.keras.models, utils=tf.keras.utils) \n",
    "    if freeze_pretrained:\n",
    "        base_model.trainable = False\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(n_classes, dtype='float16'),\n",
    "            keras.layers.Softmax()\n",
    "        ])\n",
    "    model.compile(\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setup_model(n_classes, optimizer, default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 157 steps, validate for 16 steps\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 4.0710 - acc: 0.0828 - precision_1: 0.0556 - recall_1: 4.3630e-04WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 71s 451ms/step - loss: 4.0748 - acc: 0.0824 - precision_1: 0.0435 - recall_1: 5.0000e-04 - val_loss: 3.9044 - val_acc: 0.0780 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 2/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 4.0160 - acc: 0.1000 - precision_1: 0.0610 - recall_1: 0.0011WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 4.0086 - acc: 0.1015 - precision_1: 0.0824 - recall_1: 0.0015 - val_loss: 3.9302 - val_acc: 0.0970 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 3/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.9884 - acc: 0.1104 - precision_1: 0.1227 - recall_1: 0.0029WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 61s 388ms/step - loss: 3.9870 - acc: 0.1113 - precision_1: 0.1364 - recall_1: 0.0033 - val_loss: 3.9795 - val_acc: 0.0780 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 4/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.8666 - acc: 0.1238 - precision_1: 0.3956 - recall_1: 0.0039WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 66s 422ms/step - loss: 3.8598 - acc: 0.1248 - precision_1: 0.3929 - recall_1: 0.0044 - val_loss: 4.0002 - val_acc: 0.0710 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 5/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 3.7847 - acc: 0.1339 - precision_1: 0.3727 - recall_1: 0.0045WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 67s 428ms/step - loss: 3.7843 - acc: 0.1349 - precision_1: 0.3750 - recall_1: 0.0045 - val_loss: 3.8250 - val_acc: 0.1290 - val_precision_1: 0.9000 - val_recall_1: 0.0090\n",
      "Epoch 6/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 3.7493 - acc: 0.1420 - precision_1: 0.4773 - recall_1: 0.0092WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 3.7499 - acc: 0.1432 - precision_1: 0.4764 - recall_1: 0.0091 - val_loss: 3.8293 - val_acc: 0.1250 - val_precision_1: 0.3333 - val_recall_1: 0.0050\n",
      "Epoch 7/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 5s - loss: 3.7111 - acc: 0.1521 - precision_1: 0.5631 - recall_1: 0.0136WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 68s 430ms/step - loss: 3.7163 - acc: 0.1515 - precision_1: 0.5678 - recall_1: 0.0134 - val_loss: 3.8226 - val_acc: 0.1230 - val_precision_1: 0.5455 - val_recall_1: 0.0060\n",
      "Epoch 8/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.6944 - acc: 0.1541 - precision_1: 0.5800 - recall_1: 0.0158WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 65s 416ms/step - loss: 3.6908 - acc: 0.1552 - precision_1: 0.5856 - recall_1: 0.0171 - val_loss: 3.8209 - val_acc: 0.1300 - val_precision_1: 0.3667 - val_recall_1: 0.0110\n",
      "Epoch 9/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.6663 - acc: 0.1607 - precision_1: 0.6309 - recall_1: 0.0218WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 65s 416ms/step - loss: 3.6637 - acc: 0.1610 - precision_1: 0.6416 - recall_1: 0.0213 - val_loss: 3.7653 - val_acc: 0.1380 - val_precision_1: 0.3333 - val_recall_1: 0.0160\n",
      "Epoch 10/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "144/157 [==========================>...] - ETA: 4s - loss: 3.6415 - acc: 0.1645 - precision_1: 0.5967 - recall_1: 0.0273WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "157/157 [==============================] - 65s 415ms/step - loss: 3.6419 - acc: 0.1625 - precision_1: 0.5842 - recall_1: 0.0267 - val_loss: 3.7697 - val_acc: 0.1320 - val_precision_1: 0.8889 - val_recall_1: 0.0080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd42807a710>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_image_iterator, validation_data=val_image_iterator, epochs=10, use_multiprocessing=True, workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "157/157 [==============================] - 41s 262ms/step - loss: 3.7567 - acc: 0.1340 - precision_1: 0.7345 - recall_1: 0.0083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.7567426353503186, 0.134, 0.7345133, 0.0083]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_image_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(test_image_iterator).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([test_image_iterator.class_indices[c] for c in test_metadata_df['class']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1321"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "sample_size = 11000\n",
    "val_size = 1000\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_val_metadata_df, test_metadata_df = get_metadata()\n",
    "    \n",
    "    if sample_size != 'all':\n",
    "        __, train_val_metadata_df = model_selection.train_test_split(\n",
    "            train_val_metadata_df,\n",
    "            test_size=sample_size,\n",
    "            random_state=2,\n",
    "            stratify=train_val_metadata_df['class']\n",
    "        )\n",
    "    train_metadata_df, val_metadata_df = model_selection.train_test_split(\n",
    "        train_val_metadata_df,\n",
    "        test_size=val_size,\n",
    "        random_state=2,\n",
    "        stratify=train_val_metadata_df['class']\n",
    "    )\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='logs', histogram_freq=0, write_graph=True, write_images=False,\n",
    "        update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
    "        embeddings_metadata=None\n",
    "    )\n",
    "\n",
    "    train_image_iterator, val_image_iterator, test_image_iterator = get_train_val_test_iterators(\n",
    "        train_metadata_df, val_metadata_df, test_metadata_df,\n",
    "        image_size=image_size, train_batch_size=train_batch_size, test_batch_size=test_batch_size, val_batch_size=val_batch_size) \n",
    "    n_classes = len(train_image_iterator.class_indices)\n",
    "    model = setup_model(n_classes, optimizer, default_metrics)\n",
    "    model.fit(train_image_iterator, validation_data=val_image_iterator, epochs=10, callbacks=[tensorboard_callback])\n",
    "    model.save('data/model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
